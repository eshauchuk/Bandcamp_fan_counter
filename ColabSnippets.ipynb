{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColabSnippets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOB2tT3JNet+pPYR2loAA05",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshauchuk/Bandcamp_fan_counter/blob/main/ColabSnippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My custom code snippets \n",
        "**Stored on github and loaded into any colab notebook you use** \n",
        "\n",
        "<br>\n",
        "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n"
      ],
      "metadata": {
        "id": "jSOswv4CA47N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## general commands (open, sort, count, unpack etc)\n"
      ],
      "metadata": {
        "id": "FyD7yBJMC2ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "median(), mean(),max//min() functions on lists are in the numpy\n",
        "list comprehension can have if at the end:\n",
        "odds = [i for i in range(10) if i%2 == 0]\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(“ignore\")\n",
        "\n",
        "\n",
        "# files os etc\n",
        "import sys\n",
        "print(sys.path)\n",
        "\n",
        "os.walk + next()\n",
        "#generate the dir tree and path to var's through the next() iterator\n",
        "import os\n",
        "path, dirs, files = next(os.walk(\"/content/lab1-neural-networks/letters_mod/\"))\n",
        "files.sort()\n",
        "\n",
        "#sort\n",
        "sorted(scores, key = lambda x: x[0], reverse = True)\n",
        "\n",
        "\n",
        "'ABCD'[answerOptions.index(correctAnswer)] – get the index in answerOptions list and pass it to string to pick a letter A = 0 etc \n",
        "\n",
        ".strip() string method won't touch middle for some reason, hence i wrote the re.sub module version of it and also there's simpler way: \n",
        "''.join(x for x in word if x not in 'sudontwant')\n",
        "\n",
        "#import requests #for googling \n",
        "res = requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:])) res.raise_for_status()\n",
        "\n",
        "open('u.item', encoding = \"ISO-8859-1\") #opening encoding problem\n",
        "\n",
        "round(number, num_after_dot) #rounding\n",
        "\n",
        "func.attr = value #atributes\n",
        "\n",
        "[\"foo\", \"bar\", “baz\"].index(\"bar\") # find item in the list (linear time, only first match! Throws if not present)\n",
        "\n",
        "np.where(np.isin(wine_dataset.feature_names,’proline')) # find elem in the list if multiple also probably faster\n",
        "\n",
        "#argument unpacking first right then left: \n",
        "f1, f2 = f2, f1+f2 #actually makes tuples and unpacks them\n",
        "\n",
        "#alternative to range(len(list))\n",
        "for index, item in enumerate(iterable)\n",
        "\n",
        "# count elements  or items in a list\n",
        "from collections import Counter\n",
        "genders_verbose = [gender_mapping[int(gender)] for gender in df[\"gender\"]]\n",
        "print(Counter(genders_verbose))\n",
        "\n",
        "\n",
        "# formt string \n",
        "\"p value: {:.10f}”.format(p)\n",
        "\"Shepherd %s is %d years old.\" % (shepherd, age)\n",
        "\n",
        "\n",
        "# argument unpacking for functions:\n",
        "def (param, *, param2): #enforcing keyword arg\n",
        "\n",
        "function(*list) #passing the args in a list \n",
        "function(**dictionary) #passing dict as key word args\n",
        "function(*list, **dictionary) # combined way\n",
        "\n",
        "#reserve the CamelCase for class names\n",
        "# unpack tuple to function\n",
        "[f(*xs) for xs in iterable]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# sorting, pairing, itertools\n",
        "\n",
        "def pairs(*lists):\n",
        "    \"\"\" Get unique pairs from any different lists \"\"\"\n",
        "    from itertools import combinations, product\n",
        "    for t in combinations(lists, 2):\n",
        "        for pair in product(*t):\n",
        "            yield pair\n",
        "\n",
        "# just r number of unique combination\n",
        "itertools.combinations(wine_dataset.feature_names, 2)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vvyzua6CBJmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## numpy / pandas"
      ],
      "metadata": {
        "id": "wXKOylk0DH0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy \n",
        "np.cov(X_centered, rowvar=0) # covariance matrix\n",
        "from numpy.linalg import svd, det \n",
        "np.inv vs np.pinv = pseudo inverse matrix will find the pseudo inv if determinant =0 (inv is not stable solution)\n",
        "\n",
        "# dict with counts\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "dict(zip(unique, counts))\n",
        "\n",
        "# tensor manipulations \n",
        "x[...,::,-1]  - cut one dimmension out of 4 dimmensional tensort, \n",
        "(1, 224, 224)\n",
        "(1, 224, 224, 3)\n",
        "\n",
        "# pandas tricks: \n",
        "# use .isna(); .isnull() might become deprecated??? \n",
        "dfdata[dfdata.isna().any(axis=1)] #rows with NA\n",
        "\n",
        "# nan null vis\n",
        "import missingno as msno\n",
        "msno.bar // heatmap // matrix\n",
        "\n",
        "# count types in series with object type\n",
        "df.Series.apply(type).value_counts()\n",
        "\n",
        "# normalise in multilevel\n",
        "senior_churn = telecom_users_df.groupby(['Churn', ])['SeniorCitizen'].value_counts()\n",
        "\n",
        "senior_churn.div(\n",
        "senior_churn.groupby('SeniorCitizen').sum(), level=1)\n",
        "\n",
        "\n",
        "## SELECTING / DELETING\n",
        "# columns besides specified\n",
        "dfdata.loc[:, ~ dfdata.columns.isin(['PassengerId', 'Name', 'Ticket', ‘Cabin’])]\n",
        "\n",
        "# same but more clear\n",
        "dfdata[dfdata.columns.difference(['PassengerId', 'Name', 'Ticket', 'Cabin'])]\n",
        "\n",
        "# select all but one\n",
        "df.loc[:, df.columns != ‘b’]\n",
        "\n",
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "\n",
        "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
        "                        X_train_full[cname].dtype == \"object\"]\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "\n",
        "# replace text in the column\n",
        "dataset['Title'].replace(\n",
        "['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', ‘Dona’],'Rare') \n",
        "\n",
        "# Give gender a numeric value; 0 = male, 1 = female\n",
        "titanic_df['Sex_Numeric'] = \n",
        "\t(titanic_df['Sex'].astype('category')).cat.codes\n",
        "\n",
        "# Mapping cat value depending on value range \n",
        "dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "\n",
        "# dict map, assign # Mapping titles\n",
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ".groupby(level=0).apply(lambda x: 100 * x / x.sum())\\ \n",
        "\n",
        "# неплохо переводит в проценты\n",
        "df['binned'] = pd.cut(df['lender_count'], bins=bins)\n",
        "\n",
        "#find combination of different col values:\n",
        "newstud_per_course_permodule.groupby(['module_number', ‘lesson_number’]).size().reset_index().head(15)\n",
        "\n",
        "\n",
        "# .explode if one you have value as a list\n",
        "df.explode()\n",
        "That is the same as:\n",
        "lst_col = ‘track_fans' # column that has list entry\n",
        "r = pd.DataFrame({\n",
        "      col:np.repeat(df[col].values, df[lst_col].str.len())\n",
        "      for col in df.columns.drop(lst_col)}\n",
        "    ).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns]\n",
        "\n",
        "#pd.melt - make every column an observation type\n",
        "df.melt(var_name=‘column_type', value_name=‘observation')\n",
        "\n",
        "#EDA\n",
        "df.dtypes \n",
        "\n",
        "\n",
        "\n",
        "\f# plt + sns/ matplotlib:\n",
        "\n",
        "# heatmap, df must be .corr() before feeding it to sns\n",
        "# проверяю есть ли внутренние корреляции между данными \n",
        "fig, ax = plt.subplots(figsize = (16,9))\n",
        "df_corr = telecom_users_df.corr()\n",
        "\n",
        "# маска, чтобы отобразить только половину граффика \n",
        "mask = np.triu(np.ones_like(df_corr, dtype=bool))\n",
        "mask = mask[1:, :-1]\n",
        "corr = df_corr.iloc[1:,:-1].copy()\n",
        "\n",
        "# хитмар (нет значительных негативных корреляций) \n",
        "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
        "sns.heatmap(\n",
        "    corr, mask=mask, annot=True, vmin=-1, vmax=1, \n",
        "    linewidths=5, cmap=cmap, \n",
        "    cbar_kws={\"shrink\": .2} , fmt='.2f', square=False)\n",
        "# ticks\n",
        "yticks = [i for i in corr.index]\n",
        "xticks = [i for i in corr.columns]\n",
        "plt.yticks(plt.yticks()[0], labels=yticks, rotation=0)\n",
        "plt.xticks(plt.xticks()[0], labels=xticks, rotation=75)\n",
        "\n",
        "\n",
        "# scatter matrix in sns\n",
        "sns.pairplot(edf_p, y_vars = 'feature_4', x_vars = edf_p.columns.values) #.set(xticklabels=[])\n",
        "\n",
        "# in pandas way for scatter matrix\n",
        "pd.plotting.scatter_matrix(\n",
        "features, c=targets, figsize=(25, 25), marker=‘o', hist_kwds={'bins': 20}, s=40, alpha=.8)\n",
        "\n",
        "# saving pictures, should be called in the same cell as graph plotted \n",
        "plt.savefig(‘wine.pdf') \n",
        "\n",
        "#stacking plots in succesion \n",
        "#repeat in one cell with different subplot value\n",
        "plt.figure()\n",
        "plt.subplot(211) #next can be 212\n",
        "plt.bar(range(degree), model_Ridge.coef_)\n",
        "\n",
        "\n",
        "# heatmap another \n",
        "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=plt.cm.RdBu, linecolor='white', annot=True)\n",
        "\n",
        "#image show plt\n",
        "def read_and_show(path):\n",
        "    image = plt.imread(path)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image)\n",
        "    return image\n",
        "\n"
      ],
      "metadata": {
        "id": "GKWKr5vWDJfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualisations: matplotlib, seaborn"
      ],
      "metadata": {
        "id": "Ytm9u2W9ES6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_some_samples(some_samples):\n",
        "    \"\"\" Plot an array of images\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    for j in range(some_samples.shape[0]):\n",
        "        ax = fig.add_subplot(4, 8, j+1)\n",
        "        ax.imshow(some_samples[j,:,:,0], cmap='gray')\n",
        "        plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "    \n"
      ],
      "metadata": {
        "id": "_z5ZtGgGER29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## requests / bs4 / SQL\n"
      ],
      "metadata": {
        "id": "36mD9MwdDfee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# requests, url lib, beautiful soup\n",
        "\n",
        "for link in soup.find_all('a', {'class': 'fan pic'}):\n",
        "\tfan_list.append(link.get('href')[:-15])\n",
        "\n",
        "\n",
        "# SQL connection:\n",
        "mysql -u user_name -p passwords -h host_name database_name\n",
        "\n",
        "\n",
        "https://stepik.org/lesson/297508/step/1?unit=279268\n",
        "\n",
        "installing sql connector: \n",
        "\tsudo -H python3 -m pip install mysql-connector-python\n",
        "\n",
        "#finding doc of function with __doc__ attribute \n",
        "#works if the first line in function is a str\n",
        "print(function.__doc__)\n"
      ],
      "metadata": {
        "id": "7vHok8LNDf87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearn"
      ],
      "metadata": {
        "id": "5QMk-ma-Dp-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\f# sklearn  \n",
        "# generate datasets \n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# train test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# для регрессии\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.spatial import distance\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# classifiers\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# dimension reduction\n",
        "from sklearn.decomposition import (PCA, SVD)\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "# clusterisation\n",
        "from sklearn.cluster import (KMeans, DBSCAN)\n",
        "\n",
        "\n",
        "#ensables\n",
        "from sklearn.ensemble \n",
        "\timport AdaBoostClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\txgb.DMatrix #makes this dtype for higher efficiency \n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.ensemble \n",
        "\timport (RandomForestClassifier, AdaBoostClassifier, \n",
        "\tGradientBoostingClassifier, ExtraTreesClassifier)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import \n",
        "\tprecision_recall_fscore_support\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "\n",
        "# model selection\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "##cluster metrics\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics.cluster import \n",
        "\t\t\t\t\t\tadjusted_mutual_info_score\n",
        "\n",
        "# sklearn confusion_matrix has weird axes labels for 2x2\n",
        "You can either\n",
        "\n",
        "# flatten it  \n",
        "tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
        "\n",
        "# get better looking axes \n",
        "confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "\n",
        "# preprocessing \n",
        "# Apply ordinal encoder to each column with categorical data\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "X_train[object_cols] = ordinal_encoder.fit_transform(\n",
        "\t\t\t\t\t\t\tX_train[object_cols])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = \n",
        "\t\t\t\tSimpleImputer(strategy='constant')\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "\n",
        "my_pipeline = Pipeline(\n",
        "\t\t\t\tsteps=[('preprocessor', preprocessor),\n",
        "\t\t\t\t\t\t(‘model', model)])\n",
        "\n",
        "\n",
        "# Preprocessing of training data, fit model \n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = \n",
        "\ttrain_test_split(\n",
        "\tiris_dataset_1,iris_dataset.target, \t\t\t\t\t\trandom_state = 17, \n",
        "\ttest_size = 0.2, stratify = iris_dataset.target)\n",
        "\n",
        "# xgboost\n",
        "my_model = XGBRegressor(\n",
        "\t\tn_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
        "my_model.fit(X_train, y_train, \n",
        "             early_stopping_rounds=5, \n",
        "             eval_set=[(X_valid, y_valid)], \n",
        "             verbose=False)\n",
        "\n",
        "\n",
        "print(f'X_train shape: {x_train.shape}, y_train shape: {y_train.shape},\\n'\n",
        "      f'X_test shape: {x_test.shape}, y_test shape: {y_test.shape}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UQzN-lWMCtqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tensorflow / keras"
      ],
      "metadata": {
        "id": "920ILmtCD0gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorflow\n",
        "tf.keras.utils.plot_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "a2aIcZ4GD316"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jupyter / Colab\n"
      ],
      "metadata": {
        "id": "sJxy3y6jD3KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IdhJVSqrD-ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Jupyter magik: \n",
        "%matplotlib inline\n",
        "%timeit [i+1 for i in range(10000)]\n",
        "%debug\n",
        "\n",
        "#launching from shell\n",
        "/Users/eshauchuk/opt/anaconda3/bin/jupyter_mac.command ; exit;\n",
        "\n",
        "Running Jupiter through VPN:\n",
        "\n",
        "\n",
        "\n",
        "# Colab\n",
        "# mount drive and create filepath for checkpoint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = Path(\"/content/drive/My Drive/Skillbox/model_1\")\n",
        "path.mkdir(exist_ok=True, parents=True) \n",
        "assert path.exists()\n",
        "cpt_filename = \"vggface_best_checkpoint.hdf5\"  \n",
        "cpt_path =str(path / cpt_filename)\n"
      ],
      "metadata": {
        "id": "lsKpz5rsD_Kn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}